{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "title"
   },
   "source": [
    "# Puviyan Soil Detection - Enhanced Training v5.0\n",
    "\n",
    "GPU-accelerated training with multiple dataset options:\n",
    "- **Synthetic Data**: Generated soil-like images\n",
    "- **Real Data**: Upload your own dataset\n",
    "- **Hybrid**: Combination of both\n",
    "\n",
    "**Setup:** Runtime > Change runtime type > GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "setup"
   },
   "outputs": [],
   "source": [
    "# Setup Environment\n",
    "print('Setting up Puviyan Soil Detection Training v5.0...')\n",
    "!pip install -q tensorflow matplotlib numpy tqdm pillow\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from google.colab import files\n",
    "import json\n",
    "import os\n",
    "import zipfile\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Soil type labels\n",
    "SOIL_LABELS = [\n",
    "    \"Alluvial Soil\",\n",
    "    \"Black Soil\", \n",
    "    \"Red Soil\",\n",
    "    \"Laterite Soil\",\n",
    "    \"Desert Soil\",\n",
    "    \"Saline/Alkaline Soil\",\n",
    "    \"Peaty/Marshy Soil\",\n",
    "    \"Forest/Hill Soil\"\n",
    "]\n",
    "\n",
    "# Check GPU\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(f'‚úÖ GPU Available: {gpus[0]}')\n",
    "    tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "else:\n",
    "    print('‚ö†Ô∏è No GPU detected - Training will be slow')\n",
    "\n",
    "print(f'üìä Soil types: {len(SOIL_LABELS)}')\n",
    "print('üöÄ Environment ready!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dataset_selection"
   },
   "source": [
    "## üì• Dataset Selection\n",
    "Choose your training data source below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "data_functions"
   },
   "outputs": [],
   "source": [
    "# Data loading functions\n",
    "def generate_synthetic_data(num_samples=1000):\n",
    "    \"\"\"Generate synthetic soil-like images\"\"\"\n",
    "    print(f'üé® Generating {num_samples} synthetic soil samples...')\n",
    "    \n",
    "    # Generate base soil colors\n",
    "    soil_colors = [\n",
    "        [139, 69, 19],   # Brown (general soil)\n",
    "        [160, 82, 45],   # Saddle brown\n",
    "        [210, 180, 140], # Tan (sandy)\n",
    "        [105, 105, 105], # Gray (clay)\n",
    "        [139, 90, 43],   # Dark brown\n",
    "        [222, 184, 135], # Burlywood\n",
    "        [128, 128, 0],   # Olive (organic)\n",
    "        [165, 42, 42]    # Brown (red soil)\n",
    "    ]\n",
    "    \n",
    "    X = np.zeros((num_samples, 224, 224, 3), dtype=np.uint8)\n",
    "    y = np.zeros(num_samples, dtype=np.int32)\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        # Assign soil type\n",
    "        soil_type = i % 8\n",
    "        y[i] = soil_type\n",
    "        \n",
    "        # Base color\n",
    "        base_color = soil_colors[soil_type]\n",
    "        \n",
    "        # Create image with base color\n",
    "        img = np.full((224, 224, 3), base_color, dtype=np.uint8)\n",
    "        \n",
    "        # Add texture and variation\n",
    "        noise = np.random.normal(0, 25, (224, 224, 3))\n",
    "        img = np.clip(img + noise, 0, 255).astype(np.uint8)\n",
    "        \n",
    "        # Add some patterns\n",
    "        if np.random.random() > 0.5:\n",
    "            # Add spots/particles\n",
    "            for _ in range(np.random.randint(5, 20)):\n",
    "                x_pos = np.random.randint(0, 224)\n",
    "                y_pos = np.random.randint(0, 224)\n",
    "                size = np.random.randint(2, 8)\n",
    "                color_var = np.random.randint(-50, 50, 3)\n",
    "                spot_color = np.clip(np.array(base_color) + color_var, 0, 255)\n",
    "                img[max(0, y_pos-size):min(224, y_pos+size), \n",
    "                    max(0, x_pos-size):min(224, x_pos+size)] = spot_color\n",
    "        \n",
    "        X[i] = img\n",
    "    \n",
    "    print(f'‚úÖ Generated {num_samples} synthetic samples')\n",
    "    return X, y\n",
    "\n",
    "def load_real_data():\n",
    "    \"\"\"Load real soil images from uploaded zip file\"\"\"\n",
    "    print('üì§ Please upload your soil dataset zip file')\n",
    "    print('Expected structure: soil_dataset.zip containing folders for each soil type')\n",
    "    \n",
    "    uploaded = files.upload()\n",
    "    \n",
    "    if not uploaded:\n",
    "        print('‚ö†Ô∏è No files uploaded')\n",
    "        return None, None\n",
    "    \n",
    "    zip_name = list(uploaded.keys())[0]\n",
    "    print(f'üìÇ Extracting {zip_name}...')\n",
    "    \n",
    "    # Extract zip file\n",
    "    with zipfile.ZipFile(zip_name, 'r') as zip_ref:\n",
    "        zip_ref.extractall('dataset')\n",
    "    \n",
    "    # Load images\n",
    "    X_list = []\n",
    "    y_list = []\n",
    "    \n",
    "    dataset_path = 'dataset'\n",
    "    \n",
    "    # Find all image files\n",
    "    for root, dirs, files in os.walk(dataset_path):\n",
    "        for file in files:\n",
    "            if file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                file_path = os.path.join(root, file)\n",
    "                \n",
    "                # Try to determine soil type from folder name\n",
    "                folder_name = os.path.basename(root).lower()\n",
    "                \n",
    "                # Map folder names to soil types\n",
    "                soil_type = 0  # Default to first type\n",
    "                for i, label in enumerate(SOIL_LABELS):\n",
    "                    if any(word in folder_name for word in label.lower().split()):\n",
    "                        soil_type = i\n",
    "                        break\n",
    "                \n",
    "                try:\n",
    "                    # Load and resize image\n",
    "                    img = Image.open(file_path)\n",
    "                    img = img.convert('RGB')\n",
    "                    img = img.resize((224, 224))\n",
    "                    img_array = np.array(img)\n",
    "                    \n",
    "                    X_list.append(img_array)\n",
    "                    y_list.append(soil_type)\n",
    "                except Exception as e:\n",
    "                    print(f'‚ö†Ô∏è Error loading {file_path}: {e}')\n",
    "    \n",
    "    if not X_list:\n",
    "        print('‚ùå No valid images found')\n",
    "        return None, None\n",
    "    \n",
    "    X = np.array(X_list)\n",
    "    y = np.array(y_list)\n",
    "    \n",
    "    print(f'‚úÖ Loaded {len(X)} real images')\n",
    "    print(f'üìä Class distribution: {np.bincount(y)}')\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "print('üìã Data loading functions ready!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dataset_choice"
   },
   "outputs": [],
   "source": [
    "# Dataset selection\n",
    "print('üìä Select your training dataset:')\n",
    "print('1. Synthetic data only (fast, good for testing)')\n",
    "print('2. Real data only (upload your dataset)')\n",
    "print('3. Hybrid (synthetic + real data)')\n",
    "\n",
    "choice = input('Enter your choice (1-3): ').strip()\n",
    "\n",
    "X_train = None\n",
    "y_train = None\n",
    "X_val = None\n",
    "y_val = None\n",
    "\n",
    "if choice == '1':\n",
    "    print('üé® Using synthetic data only')\n",
    "    X_all, y_all = generate_synthetic_data(5000)\n",
    "    \n",
    "elif choice == '2':\n",
    "    print('üìÇ Using real data only')\n",
    "    X_all, y_all = load_real_data()\n",
    "    \n",
    "    if X_all is None:\n",
    "        print('‚ö†Ô∏è Falling back to synthetic data')\n",
    "        X_all, y_all = generate_synthetic_data(5000)\n",
    "        \n",
    "elif choice == '3':\n",
    "    print('üîÑ Using hybrid data (synthetic + real)')\n",
    "    \n",
    "    # Generate synthetic data\n",
    "    X_syn, y_syn = generate_synthetic_data(2500)\n",
    "    \n",
    "    # Load real data\n",
    "    X_real, y_real = load_real_data()\n",
    "    \n",
    "    if X_real is not None:\n",
    "        # Combine datasets\n",
    "        X_all = np.concatenate([X_syn, X_real], axis=0)\n",
    "        y_all = np.concatenate([y_syn, y_real], axis=0)\n",
    "        print(f'‚úÖ Combined dataset: {len(X_all)} samples')\n",
    "    else:\n",
    "        print('‚ö†Ô∏è Using synthetic data only')\n",
    "        X_all, y_all = X_syn, y_syn\n",
    "        \n",
    "else:\n",
    "    print('‚ùå Invalid choice, using synthetic data')\n",
    "    X_all, y_all = generate_synthetic_data(5000)\n",
    "\n",
    "# Split data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_all, y_all, test_size=0.2, random_state=42, stratify=y_all\n",
    ")\n",
    "\n",
    "print(f'üìä Final dataset:')\n",
    "print(f'   Training: {X_train.shape[0]} samples')\n",
    "print(f'   Validation: {X_val.shape[0]} samples')\n",
    "print(f'   Classes: {len(np.unique(y_all))}')\n",
    "\n",
    "# Show sample images\n",
    "plt.figure(figsize=(15, 3))\n",
    "for i in range(8):\n",
    "    plt.subplot(1, 8, i+1)\n",
    "    # Find first sample of each class\n",
    "    idx = np.where(y_train == i)[0]\n",
    "    if len(idx) > 0:\n",
    "        plt.imshow(X_train[idx[0]])\n",
    "        plt.title(SOIL_LABELS[i], fontsize=8)\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "model"
   },
   "outputs": [],
   "source": [
    "# Create enhanced model\n",
    "def create_enhanced_model():\n",
    "    model = keras.Sequential([\n",
    "        # Input preprocessing\n",
    "        layers.Rescaling(1./255, input_shape=(224, 224, 3)),\n",
    "        \n",
    "        # Data augmentation (only during training)\n",
    "        layers.RandomFlip('horizontal'),\n",
    "        layers.RandomRotation(0.1),\n",
    "        layers.RandomZoom(0.1),\n",
    "        layers.RandomBrightness(0.1),\n",
    "        \n",
    "        # Feature extraction\n",
    "        layers.Conv2D(32, 3, activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D(2),\n",
    "        \n",
    "        layers.SeparableConv2D(64, 3, activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D(2),\n",
    "        \n",
    "        layers.SeparableConv2D(128, 3, activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D(2),\n",
    "        \n",
    "        layers.SeparableConv2D(256, 3, activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.GlobalAveragePooling2D(),\n",
    "        \n",
    "        # Classification head\n",
    "        layers.Dense(512, activation='relu'),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(256, activation='relu'),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(8, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = create_enhanced_model()\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print('üèóÔ∏è Enhanced model created!')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "train"
   },
   "outputs": [],
   "source": [
    "# Train model\n",
    "print('üöÄ Starting training...')\n",
    "\n",
    "# Callbacks\n",
    "callbacks = [\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        monitor='val_accuracy',\n",
    "        patience=10,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=5,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "# Train\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print('‚úÖ Training completed!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "export"
   },
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Training')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Training')\n",
    "plt.plot(history.history['val_loss'], label='Validation')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Convert to TFLite\n",
    "print('üîÑ Converting to TFLite...')\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save TFLite model\n",
    "model_filename = 'puviyan_soil_model.tflite'\n",
    "with open(model_filename, 'wb') as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "# Create labels JSON file for Flutter\n",
    "labels_data = {\n",
    "    \"labels\": SOIL_LABELS,\n",
    "    \"model_info\": {\n",
    "        \"input_size\": 224,\n",
    "        \"num_classes\": 8,\n",
    "        \"model_type\": \"soil_classification\",\n",
    "        \"version\": \"5.0\",\n",
    "        \"dataset_type\": choice\n",
    "    }\n",
    "}\n",
    "\n",
    "labels_filename = 'labels.json'\n",
    "with open(labels_filename, 'w') as f:\n",
    "    json.dump(labels_data, f, indent=2)\n",
    "\n",
    "print(f'‚úÖ Model saved as {model_filename}')\n",
    "print(f'üìè Model size: {len(tflite_model) / 1024:.1f} KB')\n",
    "print(f'üìã Labels saved as {labels_filename}')\n",
    "\n",
    "# Download both files\n",
    "files.download(model_filename)\n",
    "files.download(labels_filename)\n",
    "\n",
    "print('üéâ Training complete! Both files downloaded successfully!')\n",
    "print('Ready for Flutter integration!')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
