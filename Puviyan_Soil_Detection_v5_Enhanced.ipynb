{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "soil_detection_title"
   },
   "source": [
    "# üå± Puviyan Soil Detection - Enhanced Training (v5.0)\n",
    "\n",
    "## üöÄ Key Features\n",
    "- **GPU-accelerated** training (up to 10x faster)\n",
    "- **Multiple dataset options**: Synthetic, Real, or Hybrid\n",
    "- **Enhanced model architecture** for better accuracy\n",
    "- **TFLite export** for Flutter deployment\n",
    "\n",
    "## üõ† Setup Instructions\n",
    "1. Go to **Runtime** ‚Üí **Change runtime type** ‚Üí Select **GPU**\n",
    "2. Run all cells in order\n",
    "3. Follow prompts for dataset selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "setup_environment"
   },
   "outputs": [],
   "source": [
    "# @title üöÄ Setup and Configuration\n",
    "print(\"Setting up environment...\")\n",
    "!nvidia-smi  # Check GPU status\n",
    "!pip install -q tensorflow tensorflow-model-optimization matplotlib numpy tqdm\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "from google.colab import files\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Configuration\n",
    "CONFIG = {\n",
    "    'model_name': 'soil_classifier_enhanced_v5',\n",
    "    'input_size': 224,\n",
    "    'num_classes': 8,\n",
    "    'batch_size': 32,\n",
    "    'epochs': 100,\n",
    "    'learning_rate': 0.0005,\n",
    "    'patience': 15,\n",
    "    'min_delta': 1e-4\n",
    "}\n",
    "\n",
    "# Soil type labels\n",
    "SOIL_LABELS = {\n",
    "    0: \"Alluvial Soil\",\n",
    "    1: \"Black Soil\",\n",
    "    2: \"Red Soil\",\n",
    "    3: \"Laterite Soil\",\n",
    "    4: \"Desert Soil\",\n",
    "    5: \"Saline/Alkaline Soil\",\n",
    "    6: \"Peaty/Marshy Soil\",\n",
    "    7: \"Forest/Hill Soil\"\n",
    "}\n",
    "\n",
    "# Setup GPU\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(f\"‚úÖ GPU Detected: {tf.test.gpu_device_name()}\")\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No GPU detected. Training will be very slow!\")\n",
    "    print(\"   Go to Runtime > Change runtime type > Select GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "data_loading"
   },
   "source": [
    "## üì• Data Loading Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "data_loading_code"
   },
   "outputs": [],
   "source": [
    "# @title üîÑ Data Loading Functions\n",
    "def generate_synthetic_data(num_samples=1000):\n",
    "    \"\"\"Generate synthetic soil images for testing\"\"\"\n",
    "    print(f\"üé® Generating {num_samples} synthetic soil samples...\")\n",
    "    \n",
    "    # Generate random images with soil-like patterns\n",
    "    X = np.random.randint(50, 200, (num_samples, CONFIG['input_size'], CONFIG['input_size'], 3), dtype=np.uint8)\n",
    "    \n",
    "    # Add some texture patterns\n",
    "    for i in range(num_samples):\n",
    "        # Add noise for texture\n",
    "        noise = np.random.normal(0, 20, X[i].shape)\n",
    "        X[i] = np.clip(X[i] + noise, 0, 255).astype(np.uint8)\n",
    "    \n",
    "    # Generate balanced labels\n",
    "    y = np.random.randint(0, CONFIG['num_classes'], num_samples, dtype=np.int32)\n",
    "    \n",
    "    print(f\"‚úÖ Generated {num_samples} synthetic samples\")\n",
    "    return X, y\n",
    "\n",
    "def upload_real_data():\n",
    "    \"\"\"Upload and process real soil images\"\"\"\n",
    "    from google.colab import files\n",
    "    import zipfile\n",
    "    \n",
    "    print(\"üì§ Please upload your dataset zip file\")\n",
    "    uploaded = files.upload()\n",
    "    \n",
    "    if not uploaded:\n",
    "        print(\"‚ö†Ô∏è No files uploaded. Using synthetic data.\")\n",
    "        return None, None\n",
    "    \n",
    "    zip_name = list(uploaded.keys())[0]\n",
    "    print(f\"üìÇ Extracting {zip_name}...\")\n",
    "    \n",
    "    with zipfile.ZipFile(zip_name, 'r') as zip_ref:\n",
    "        zip_ref.extractall('dataset')\n",
    "    \n",
    "    # TODO: Add your data loading code here\n",
    "    print(\"üìÇ Real data loading not implemented yet. Using synthetic data.\")\n",
    "    return None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "model_architecture"
   },
   "source": [
    "## üèóÔ∏è Enhanced Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "model_architecture_code"
   },
   "outputs": [],
   "source": [
    "# @title üß† Create Enhanced Model\n",
    "def create_enhanced_model():\n",
    "    \"\"\"Create the enhanced soil classification model\"\"\"\n",
    "    print(\"üèóÔ∏è Creating enhanced model architecture...\")\n",
    "    \n",
    "    inputs = keras.Input(shape=(CONFIG['input_size'], CONFIG['input_size'], 3))\n",
    "    \n",
    "    # Data augmentation and preprocessing\n",
    "    x = layers.Rescaling(1./255)(inputs)\n",
    "    x = layers.RandomFlip(\"horizontal\")(x)\n",
    "    x = layers.RandomRotation(0.2)(x)\n",
    "    x = layers.RandomZoom(0.2)(x)\n",
    "    x = layers.RandomBrightness(0.1)(x)\n",
    "    \n",
    "    # Feature extraction with depthwise separable convolutions\n",
    "    x = layers.Conv2D(32, 3, activation='relu', padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling2D()(x)\n",
    "    \n",
    "    x = layers.SeparableConv2D(64, 3, activation='relu', padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling2D()(x)\n",
    "    \n",
    "    x = layers.SeparableConv2D(128, 3, activation='relu', padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling2D()(x)\n",
    "    \n",
    "    x = layers.SeparableConv2D(256, 3, activation='relu', padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    \n",
    "    # Classification head\n",
    "    x = layers.Dense(512, activation='relu')(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    x = layers.Dense(256, activation='relu')(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    outputs = layers.Dense(CONFIG['num_classes'], activation='softmax')(x)\n",
    "    \n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=CONFIG['learning_rate']),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Model created with {model.count_params():,} parameters\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "training_section"
   },
   "source": [
    "## üöÄ Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "training_code"
   },
   "outputs": [],
   "source": [
    "# @title üèÉ Start Training\n",
    "def train():\n",
    "    \"\"\"Main training function\"\"\"\n",
    "    # Select dataset type\n",
    "    print(\"üìä Select dataset type:\")\n",
    "    print(\"1. Synthetic data (generated)\")\n",
    "    print(\"2. Upload real data\")\n",
    "    print(\"3. Hybrid (synthetic + real)\")\n",
    "    \n",
    "    choice = input(\"Enter your choice (1-3): \").strip()\n",
    "    \n",
    "    # Load data based on choice\n",
    "    if choice == '1':\n",
    "        X, y = generate_synthetic_data(num_samples=5000)\n",
    "    elif choice == '2':\n",
    "        X, y = upload_real_data()\n",
    "        if X is None:\n",
    "            print(\"‚ö†Ô∏è Using synthetic data as fallback\")\n",
    "            X, y = generate_synthetic_data(num_samples=5000)\n",
    "    elif choice == '3':\n",
    "        X_syn, y_syn = generate_synthetic_data(num_samples=2500)\n",
    "        X_real, y_real = upload_real_data()\n",
    "        if X_real is not None:\n",
    "            X = np.concatenate([X_syn, X_real], axis=0)\n",
    "            y = np.concatenate([y_syn, y_real], axis=0)\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è Using synthetic data only\")\n",
    "            X, y = X_syn, y_syn\n",
    "    else:\n",
    "        print(\"‚ùå Invalid choice. Using synthetic data by default.\")\n",
    "        X, y = generate_synthetic_data(num_samples=5000)\n",
    "    \n",
    "    print(f\"üìä Dataset: {X.shape[0]} samples, {len(np.unique(y))} classes\")\n",
    "    \n",
    "    # Create and train model\n",
    "    model = create_enhanced_model()\n",
    "    \n",
    "    # Callbacks\n",
    "    callbacks = [\n",
    "        keras.callbacks.EarlyStopping(\n",
    "            monitor='val_accuracy',\n",
    "            patience=CONFIG['patience'],\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        keras.callbacks.ModelCheckpoint(\n",
    "            f\"{CONFIG['model_name']}.h5\",\n",
    "            save_best_only=True,\n",
    "            monitor='val_accuracy',\n",
    "            verbose=1\n",
    "        ),\n",
    "        keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=5,\n",
    "            verbose=1\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    # Train\n",
    "    print(\"\\nüöÄ Starting training...\")\n",
    "    history = model.fit(\n",
    "        X, y,\n",
    "        validation_split=0.2,\n",
    "        batch_size=CONFIG['batch_size'],\n",
    "        epochs=CONFIG['epochs'],\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Plot training history\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Model Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Convert to TFLite\n",
    "    print(\"\\nüîÑ Converting to TFLite...\")\n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "    tflite_model = converter.convert()\n",
    "    \n",
    "    # Save TFLite model\n",
    "    tflite_filename = f\"{CONFIG['model_name']}.tflite\"\n",
    "    with open(tflite_filename, 'wb') as f:\n",
    "        f.write(tflite_model)\n",
    "    \n",
    "    print(f\"‚úÖ Model saved as {tflite_filename}\")\n",
    "    print(f\"üìè TFLite model size: {len(tflite_model) / 1024:.1f} KB\")\n",
    "    \n",
    "    # Download the model\n",
    "    files.download(tflite_filename)\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "# Start training\n",
    "model, history = train()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],\n   "gpuType": "T4"\n  },\n  "kernelspec": {\n   "display_name": "Python 3",\n   "name": "python3"\n  },\n  "language_info": {\n   "name": "python"\n  },\n  "accelerator": "GPU"\n },\n "nbformat": 4,\n "nbformat_minor": 0\n}
